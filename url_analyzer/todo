# Prioritized TODO



- run tests with a bunch of example phishing pages from openphish and validate that the results are good

- add a simple API-key based rate limit


- change page design to include description of what the page does 
- write an API documentation page and link it at the top of the website

- switch to a SQL data storage scheme to enable rate limiting

- build an authentication engine with user creation
  - JWT is saved to a file
  - API keys are created from JWT when new users are created
  - Per-API key quota is tracked in a database


# Security Requirements
- move lightsail DNS to proxy through zero-phishing


# Performance Backlog
- change the classifier to be multiclass (phishing, suspicious, safe) to cover stuff like mail1.pages.dev
- try switching to Gemini Flash as a simple free option https://ai.google.dev/pricing
- integrate with webarena schema, especially viewport visible HTML https://github.com/web-arena-x/webarena/blob/1469b7c9d8eaec3177855b3131569751f43a40d6/browser_env/processors.py#L174
- modify the API to enable spidering as well



# UX Backlog


# Product Backlog
- Create a lambda function that does some kind of basic scanning (maybe just openphish download) to get a list of malicious urls and write them to s3
- Add a splunk integration that enables scanning links that match certain criteria in splunk against this scanner. Maybe via https://github.com/splunk/attack_range
- Add a hosted nested browser to lightsail


yy- move JWT secret key storage to secrets manager
xx- add a basic end-to-end local testing suite to run before pushing to lightsail
xx- develop a cleaner failure mode for urls that don't resolve like test.com
xx- fix docker deployment to lightsail
xx- add better error handling for SSL failures
xx- add LLM insights about whether the page is a "domain not registered" page
xx- Hook up a certstream script that logs all recently issues certificates and checks them for phishing
xxx- figure out how to capture popups etc to prevent screenshots of the wrong page
xx- switch from a full html encoding to just grabbing the relevant parts of the html (especially forms)
xx- pipe a description of the image to the LLM classification logic
xx- add IP and domain intelligence to the prompt, http response, and page display
xx- improve user display to show a cleaner organized output
xx- increase speed of the logic to fetch the HTML+Image by refactoring the search logic to just grab the page rather than run a full spider
xx- switch the deployed vercel page to be zerophishing.com
xx- add on http:// to urls that don't have them automatically in React
xx- change logic from `is_phishing` and `page_state` to a single three-class classification task (safe, phishing, inactive) and verify that the classification works for all of the test pages. Also give a lot of data in the prompt about the definition and scope of these categories
